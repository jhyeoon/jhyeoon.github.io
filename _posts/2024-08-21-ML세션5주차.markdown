---
layout: post
title:  "ML세션 5주차"
date:   2024-08-21 19:37:24 +0800
categories: jekyll update
published: true
---

**# 혼자 공부하는 머신러닝+딥러닝 Ch. 4**

# Ch.5 트리 알고리즘

## **5-1 결정 트리**

### **로지스틱 회귀로 와인 분류하기**

**1. 와인 데이터 불러오기**

```python
import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')
wine.head()
```

<img width="160" alt="image" src="https://github.com/user-attachments/assets/590ef23d-7e95-4dc6-95ac-d835658f8898">

**2. 판다스 df를 넘파이 배열로 바꾸기**

```python
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
```

**3.훈련 세트와 테스트 세트로 나누기**

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)
```

- `test_size=0.2`: 샘플의 20%만 테스트 세트로 나눔
    - 기본값은 25%

**4. 특성 표준화해서 전처리하기**

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

**5. 로지스틱 회귀 모델 훈련하기**

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

> 0.7808350971714451
> 0.7776923076923077

&rarr; 과소적합

**6. 결정계수와 절편 출력하기**

```python
print(lr.coef_, lr.intercept_)
```

> [[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]

&rarr; z = 0.51270274 x (알코올 도수) + 1.6733911 x (당도) - 0.68767781 x (pH) + 1.81777902, if z>0 &rarr; 화이트 와인, if z<0 &rarr; 레드 와인
&rarr; 설명하기 어려움
&rarr; 로지스틱 회귀 대신 결정 트리

### **결정 트리**
: 예/아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘
- 노드: 훈련 데이터의 특성에 대한 테스트 표현 
    - 루트 노드: 맨 위의 노드
    - 리프 노드: 맨 아래의 노드
- 가지: 테스트의 결과 (True/False)를 나타냄
- 예측 클래스: 노드에서 더 많은 샘플이 있는 클래스로 예측

**1. 결정 트리 훈련하기**

```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```

> 0.996921300750433
> 0.8592307692307692

&rarr; 과대적합
&rarr; 트리가 제한 없이 자라났기 때문에

- **DecisionTreeClassifier()**
: 결정 트리 분류 클래스
    - `criterion` 매개변수: 불순도 지정
        - 기본값은 'gini'
        - 'entropy'

    - `splitter` 매개변수: 노드 분할하는 전략 선택
        - 기본값은 'best': 정보 이득이 최대가 되도록
        - 'random'이면 임의로 노드 분할
    
    - `max_depth` 매개변수: 트리가 성장할 최대 깊이 지정
        - 기본값은 'None': 리프 노드가 순수하거나 min_smaples_split보다 샘플 개수가 적을 때까지 성장

    - `max_samples_split` 매개변수: 노드를 나누기 위한 최소 샘플 개수
        - 기본값은 2

    - `max_features` 매개변수: 최적의 분할을 위해 탐색할 특성의 개수 지정
        - 기본값은 'None'

**2. 결정 트리 그림으로 나타내기**

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```
<img width="604" alt="image" src="https://github.com/user-attachments/assets/46169ee9-9628-431b-9c89-9b9e4db7f4c8">

<img width="164" alt="image" src="https://github.com/user-attachments/assets/7206dc84-39d6-4d2a-ae7e-a7a4ff536a25">

**3. 결정 트리의 루트 노드와 아래의 노드 그리기**

```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

- **plot_tree()**
: 결정 트리 모델 시각화
    - `max_depth=1`: 루트 노드와 아래의 노드 1개를 그림
    - `filled=True`: 클래스에 맞게 색 칠하기
        - value = []의 왼쪽이 음성 클래스, 오른쪽이 양성 클래스
        - 음성 클래스가 많으면 붉은색, 양성 클래스가 많으면 푸른색
        - 특정 클래스의 비율이 높아지면 점점 진한 색
    - `feature_names=['alcohol', 'sugar', 'pH']`: 특성의 이름 전달

### **지니 불순도(gini)**

- DecisionTreeClassifier()의 `criterion`매개변수의 기본값이 'gini'
    - `critierion` 매개변수: 노드에서 데이터를 분할할 기준을 정하는 것
    - `criterion='entropy'`
        : 엔트로피 불순도 = - 음성 클래스 비율 x log_2(음성 클래스 비율) - 양성 클래스 비율 x log_2(양성 클래스 비율)
: *지니 불순도 = 1 - (음성 클래스 비율^2 +양성 클래스 비율^2)*

- 최악의 지니 불순도 = 0.5
    &rarr; 노드의 두 클래스의 비율이 1/2씩일 경우
    &rarr; 1 - ((50/100)^2 + (50/100)^2) = 0.5
    <img width="284" alt="image" src="https://github.com/user-attachments/assets/4cd00de7-47e1-4bd1-8cc9-eff96ae20644">

- **정보 이득**
: 부모 노드와 자식 노드의 불순도 차이
    - *부모의 불순도 - (왼쪽 노드 샘플 수/부모의 샘플 수) x 왼쪽 노드 불순도 - (오른쪽 노드 샘플 수/부모의 샘플 수) x 오른쪽 노드 불순도*
    - 결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록 트리를 성장시킴

### **가지치기**

: 루트 노드 아래로 몇개의 노드까지만 성장하게 함
&rarr; 과대적합 방지


**1. 가지쳐서 결정 트리 모델 훈련하기**

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```

> 0.8454877814123533
> 0.8415384615384616

&rarr; 훈련 세트의 점수는 낮아졌지만, 테스트 세트 점수는 그대로

**2. 가지친 결정 트리 모델 그리기**

```python
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

<img width="664" alt="image" src="https://github.com/user-attachments/assets/5dcddd9d-b481-4fc2-b994-f688bf5abb20">

&rarr; but 특성값이 표준점수로 바뀐 상태이기 때문에 이해하기 어려움
ex) sugar <= -0.239

### **결정 트리에서의 전처리**

: 결정 트리에서 가지를 나누는 기준은 불순도
&rarr; 불순도는 클래스별 비율로 계산하기 때문에 특성값의 스케일과 무관
&rarr; 표준화 전처리할 필요x

**1. 전처리하지 않은 훈련/테스트 세트로 결정 트리 모델 훈련하기**

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))
```

> 0.8454877814123533
> 0.8415384615384616

&rarr; 훈련/테스트 세트의 점수가 전처리한 세트의 결과와 동일

**2. 전처리하지 않은 모델 그리기**

```python
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

<img width="665" alt="image" src="https://github.com/user-attachments/assets/579ac555-e7c7-4c59-9cfb-f2df09681d5e">

&rarr; 특성값이 원래의 스케일대로 나타나서 이해하기 쉬움
ex) sugar <= 4.325

**3. 특성 중요도 출력하기**

```python
print(dt.feature_importances_)
```

> [0.12345626 0.86862934 0.0079144 ]

- **중요도**
: 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도
    - \sum (각 노드의 정보 이득) x (전체 샘플에 대한 비율)
