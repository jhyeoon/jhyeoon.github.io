---
layout: post
title:  "ML세션 4주차"
date:   2024-08-14 19:37:24 +0800
categories: jekyll update
published: true
---

**# 혼자 공부하는 머신러닝+딥러닝 Ch. 4**

# Ch.4 다양한 분류 알고리즘

## **4-1 로지스틱 회귀**

### **k-최근접 이웃 분류기**
: k-최근접 이웃으로 주변 이웃을 찾고, 이웃 클래스의 비율을 확률로 출력  

<img width="241" alt="image" src="https://github.com/user-attachments/assets/2919c8e7-8d1a-4572-849c-c4cce57b6db5">

**1. 생선 데이터 불러오기**

```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```
<img width="311" alt="image" src="https://github.com/user-attachments/assets/5e8e960d-fe8c-414c-81a1-9a9a3a39d361">

- **.read_csv**
: csv를 df로 변환해주는 메서드

**2. 타깃 데이터의 종류 개수 확인하기**
```python
print(pd.unique(fish['Species']))
```

> ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']

&rarr; 클래스가 7개
&rarr; 다중 분류: 타깃 클래스가 2개 이상인 분류 문제

**3. 생선 데이터에서 입력 데이터 뽑아내기**

```python
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
```

- **.to_numpy()**
: df를 넘파이 배열로 변환해주는 메서드

**4. 생선 데이터에서 타깃 데이터 뽑아내기**

```python
fish_target = fish['Species'].to_numpy()
```

**5. 훈련 세트와 테스트 세트로 나누기**
```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```

**6. 훈련 세트와 테스트 세트 표준화 전처리하기**
```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

**7. k-최근접 이웃 분류기 성능 평가하기**
```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))
```

> 0.8907563025210085
> 0.85

(일단 이 예제에서는 클래스 확률을 배우는 것이 목적이므로 점수는 일단 잊어라)

**8. kn 모델의 클래스 개수 확인하기**

```python
print(kn.classes_)
```

> ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']

- .unique(fish[Species]) vs .classes_
: 후자는 전자와 달리 알파벳순으로 나열하고, 이미 타깃이 무슨 열인지 알고 있음

**9. 테스트 세트에 있는 처음 5개 샘플의 타깃값 예측해보기**
```python
print(kn.predict(test_scaled[:5]))
```

> ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']

**10. 테스트 세트 샘플의 클래스별 확률값 출력하기**

```python
import numpy as np

proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))
```

[[0.     0.     1.     0.     0.     0.     0.    ]
 [0.     0.     0.     0.     0.     1.     0.    ]
 [0.     0.     0.     1.     0.     0.     0.    ]
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]]

 - **.predict_proba()**
: 클래스별 확률값 반환
    - `test_scaled[:5]`: test_scaled 데이터셋의 인덱스 0~4

- **.round()**
: 기본적으로 소수점 첫째 자리에서 반올림
    - `decimals=4` 매개변수: 소수점 아래 4자리까지

**11.네 번째 샘플의 최근접 이웃의 클래스 확인해보기**

```python
distances, indexes = kn.kneighbors(test_scaled[3:4])
print(train_target[indexes])
```

> [['Roach' 'Perch' 'Perch']]

- **.kneighbors()**
: 주어진 데이터 포인트 k개의 최근접 이웃을 찾는 메서드, 이웃들의 거리와 인덱스 반환
    - `test_scaled[3:4]`: test_scaled 데이터셋의 인덱스 3
    - 즉, 여기에서는 indexes 변수에는 3이 저장됨

### **로지스틱 회귀**
: 선형 방정식을 사용한 분류 알고리즘
- 선형 회귀처럼 선형 방정식을 학습 <br>
    ex) z = a x (Weight) + b x (Length) + c x (Diagonal) + d x (Height) + e x (Width) + f
- 선형 회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용하여 클래스 확률 출력 가능
    - 시그모이드 함수(로지스틱 함수)
    : 선형 방정식의 출력을 0과 1 사이의 값으로 압축하며 이진 분류를 위해 사용
        - 이진 분류: 입력값에 따라 모델이 분류한 카테고리가 두 가지인 분류 알고리즘
        - 출력이 0.5보다 크면 양성 클래스(1), 0.5보다 작으면 음성 클래스(0)로 판단

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    z = np.arange(-5, 5, 0.1) # z 범위가 -5~5, 간격이 0.1
    phi = 1 / (1 + np.exp(-z))

    plt.plot(z, phi)
    plt.xlabel('z')
    plt.ylabel('phi')
    plt.show()
    ```

    <img width="422" alt="image" src="https://github.com/user-attachments/assets/4a083a28-2f39-4d1f-8519-e658424529d9">

    - 소프트맥스 함수
    : 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만듦
        - 다중 분류: 입력값에 따라 모델이 분류한 카테고리가 세 가지 이상인 분류 알고리즘
        - 정규화된 지수 함수
        - *e_sum = e^z1 + e^z2 + e^z3 + e^z4 + e^z5 + e^z6 + e^z7*
        - s1 = e^z1/e_sum, s2 = e^z2/e_sum,..., s7 = e^z7/e_sum

    <img width="259" alt="image" src="https://github.com/user-attachments/assets/4437b56a-2751-446f-9655-eb7961d43258">

### **로지스틱 회귀로 이진 분류 수행하기**

- 불리언 인덱싱
: 넘파이 배열 True, False 값을 전달

```python
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])
```

> 첫번째, 세번째 원소만 True 전달해서 'A', 'C'만 골라내기

**1. 훈련 세트의 타깃 데이터에서 도미와 빙어 골라내기**

```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```
- `train_target == 'Bream'`: train_target에서 도미는 True, 나머지는 False로 반환
- `train_target == 'Smelt'`: train_target에서 빙어는 True, 나머지는 False로 반환
- '|' : 비트 OR 연산자로 합치기

**2. 로지스틱 회귀 모델 훈련하기**

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```

**3. 훈련 세트에 있는 처음 5개 샘플의 타깃값 예측해보기**

```python
print(lr.predict(train_bream_smelt[:5]))
```

> ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']

**4. 훈련 세트 샘플의 클래스별 확률값 출력하기**

```python
print(lr.predict_proba(train_bream_smelt[:5]))
```

> [[0.99759855 0.00240145]
 [0.02735183 0.97264817]
 [0.99486072 0.00513928]
 [0.98584202 0.01415798]
 [0.99767269 0.00232731]]

 **5. 클래스 정보(음성 클래스/양성 클래스) 확인하기**

 ```python
 print(lr.classes_)
 ```

 > ['Bream' 'Smelt']
 > 음성 클래스(0): Bream, 양성 클래스(1): Smelt
 (알파벳 순으로 정렬)

**6. 로지스틱 회귀가 학습한 계수 확인하기**

```python
print(lr.coef_, lr.intercept_)
```

> [[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
> *z = -0.404 x (Weight) - 0.576 x (Length) - 0.663 x (Diagonal) - 1.013 x (Height) - 0.732 x (Width) -2161*

**7. z값 계산하기**

```python
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)
```

> [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]

- **decision_function()**
: 양성 클래스에 대한 z값 반환

**8. 시그모이드 함수에 z값 넣고 확률 구하기**

```python
from scipy.special import expit

print(expit(decisions))
```

> [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]

- **expit()**
: 사이파이 라이브러리의 시그모이드 함수
    - 양성 클래스에 대한 확률 반환

### **로지스틱 회귀로 다중 분류 수행하기**

- 로지스틱 회귀
    - L2 규제
    - C 매개변수로 규제 제어
        - 기본값 1
        - 릿지 회귀의 alpha와 반대로 C가 작을수록 규제가 커짐

**1. 로지스틱 회귀로 다중 분류 모델 훈련하기**

```python
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
```

**2. lr 모델 평가하기**

```python
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

> 0.9327731092436975
> 0.925
&rarr; 모델 good

**3. 테스트 세트의 처음 5개 샘플의 타깃값 예측해보기**

```python
print(lr.predict(test_scaled[:5]))
```

> ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']

**4. 테스트 세트 샘플의 클래스별 확률값 출력하기**

```python
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
```

[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]

 **5. 클래스 정보 확인하기**

 ```python
 print(lr.classes_)
 ```

 > ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']

 **6. 로지스틱 회귀가 학습한 계수의 크기 확인하기**

 ```python
 print(lr.coef_.shape, lr.intercept_.shape)
 ```

> (7, 5) (7,)

- 다중 분류는 클래스마다 z값을 하나씩 계산함
- 가장 높은 z값을 출력하는 클래스 &larr; 예측 클래스
- 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환

**7. z값 계산하기**

```python
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))
```

[[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
 [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
 [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
 [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
 [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]

 **8. 소프트맥스 함수에 z값 넣고 확률 구하기**

```python
from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```

- `axis=1`: 각 행(샘플)에 대해 소프트 맥스 계산
    - axis 매개변수를 지정하지 않으면 &larr; 배열 전체에 대해 소프트맥스를 계산

## **4-2 확률적 경사 하강법**

### 점진적 학습(온라인 학습)
: 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련

### 확률적 경사 하강법(SGD)
: 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘
1) 훈련 세트에서 랜덤하게 하나의 샘플을 선택 &rarr; 가장 가파른 경사를 조금 내려감
2) 훈련 세트에서 랜덤하게 또 다른 샘플을 하나 선택 &rarr; 가장 경사를 조금 내려감
3) 전체 샘플을 모두 사용할 때까지 계속함
4) 만약 다 내려오지 못했으면 훈련 세트에 모든 샘플을 다시 채워넣고 다시 시작
- 점진적 학습 알고리즘 중 하나
&rarr; 훈련 데이터가 모두 준비되어 있지 않고 계속 업데이트되어도 처음부터 다시 시작할 필요없이 학습을 이어나갈 수 있음
- **미니배치 경사 하강법**
    - 샘플을 하나씩 사용x, 여러 개 사용o
    - 실전에서 많이 사용
- **배치 경사 하강법**
    - 한 번에 전체 샘플 사용
    - 전체 데이터를 사용하기 떄문에 가장 안정적인 방법
    - 컴퓨터 자원을 많이 사용하게 됨, 한 번에 전체 데이터를 모두 읽을 수 없을지도
- **에포크**: 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정

<img width="364" alt="image" src="https://github.com/user-attachments/assets/77bb1d66-d92f-43be-836c-0d8ccf0ba166">

### 손실 함수

: 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준
- 어떤 값이 최소인지는 알지 못하므로 &rarr; 가능한 많이 찾아보고 만족할만하면 그냥 인정해야 함
- 손실 함수의 값 &darr; 좋음
- 경사 하강법을 사용할 때, 아주 조금씩 내려오므로 산의 경사면은 연속적이어야 함(미분 가능)
&rarr; 클래스별 예측 확률 사용

<img width="315" alt="image" src="https://github.com/user-attachments/assets/c67ac7e8-552c-45f5-8d16-3e8cfe326864">

### 로지스틱 손실 함수
= 이진 크로스엔트로피 손실 함수
- 이진 분류

ex) 로지스틱 이진 분류
0) 샘플 4개의 예측 확률이 0.9, 0.3, 0.2, 0.8이라고 가정
1) 첫번째 심플의 예측: 0.9, 양성클래스(1)
0.9 x 1 &rarr; -0.9
2) 두번째 샘플의 예측: 0.3, 양성클래스(1)
0.3 x 1 &rarr; -0.3
3) 세번째 샘플의 예측: 0.2, 음성클래스(0)
(1-0.2) x 1 &rarr; -0.8
4) 네번째 샘플의 예측: 0.8, 음성클래스(0)
(1-0.8) x 1 &rarr; -0.2

&rarr; 로지스틱 손실 함수 L = -(ylog(a)+(1-y)log(1-a)) (y=0, 1(음성/양성))

### 크로스엔트로피 손실 함수
- 다중 분류

### 확률적 경사 하강법 예제

**1. 입력 데이터와 타깃 데이터 가져오기**

```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
```

**2. 훈련 세트와 테스트 세트로 나누기**

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```

**3. 훈련 세트와 테스트 세트의 특성 표준화 전처리하기**

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

**4. 확률적 경사 하강법으로 모델 훈련하기**

```python
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
```

**5. sc 모델 성능 평가하기**

```python
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

> 0.773109243697479
> 0.775
&rarr; 훈련 세트와 테스트 세트 점수가 낮음
&rarr; 아마 max_iter 값이 작아서?

**6. sc 모델 이어서 훈련하기**

```python
sc.partial_fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```
> 0.8151260504201681
> 0.85
&rarr; 에포크를 한 번 더 실행하니 정확도 &uarr;

- **partial_fit()**
: SGDclassifier는 객체에 훈련 세트 전체를 전달해도 1개씩 샘플을 꺼내어 경사 하강법 수행
&rarr; 배치 경사 하강법이 아닌 확률적 경사 하강법이 맞음

### 에포크의 과대/과소 적합

- 에포크 횟수가 적으면 &rarr; 과소적합 (산은 다 내려오지 못함)
- 에포크 횟수가 많으면 &rarr; 과대적합

<img width="227" alt="image" src="https://github.com/user-attachments/assets/65ccf78b-c2d4-45e5-829f-5917f42f6776">

- 테스트 점수가 감소하기 시작하는 지점 &rarr; 과대적합 시작되는 지점 &rarr; **조기 종료**해야됨

### 최적의 에포크 횟수 찾기

**1. 그래프 준비하기**

```python
import numpy as np

sc = SGDClassifier(loss='log_loss', random_state=42)

train_score = []
test_score = []

classes = np.unique(train_target)
```

**2. 300번의 에포크 동안 훈련 반복하고, 훈련 세트와 테스트 세트 점수 리스트 만들기**

```python
for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)

    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
```

**3. 300번의 에포크 동안의 훈련 세트와 테스트 세트 점수 그래프로 그리기**

```python
import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

<img width="422" alt="image" src="https://github.com/user-attachments/assets/1e715a37-3770-46b9-b325-77143a010429">

&rarr; 에포크 횟수 100이 적절함

**4. 에포크 횟수 100으로 맞추고 모델 훈련하고, 점수 출력하기**

```python
sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

> 0.957983193277311
> 0.925

- `loss='log_loss'`: 기본값은 'hinge'

- `tol=None`: 향상될 최솟값 지정
    - 원래 일정 에포크 동안 성능이 향상되지 않으면 더 훈련x, 자동으로 멈춤
    - None으로 지정하면 자동으로 멈추지 않고 무조건 max_iter=100만큼 반복














