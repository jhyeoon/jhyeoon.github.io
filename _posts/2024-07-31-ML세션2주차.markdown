---
layout: post
title:  "ML세션 2주차"
date:   2024-07-31 20:47:24 +0900
categories: jekyll update
---

**# 혼자 공부하는 머신러닝+딥러닝 Ch. 1, 2**

# Ch.1 나의 첫 머신러닝

## 1-1 인공지능과 머신러닝, 딥러닝

### 인공지능

: 사람처럼 학습히고 추론할 수 있는 기능을 가진 컴퓨터 시스템을 만드는 기술
1. 인공일반지능(강인공지능)
- 사람과 구분하기 어려운 지능을 가진 컴퓨터 시스템
2. 약인공지능
- 특정 분야에서 사람의 일을 도와주는 보조 역할만 가능한 컴퓨터 시스템

### 머신러닝

: 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야
- 인공지능의 하위 분야 중 '지능'을 구현하기 위한 소프트웨어 담당
- 오픈소스 통계 소프트웨어 R
- 머신러닝 라이브러리 사이킷런(scikit-learn)
    - 파이썬 API 사용
    - 검증된 알고리즘만 포함되어있으므로 안정적이고 성능이 검증됨

### 딥러닝

: 머신러닝 알고리즘 중 인공 신경망을 기반으로 한 방법들을 통칭함
- 딥러닝 라이브러리 텐서플로(TensorFlow) by 구글
- 딥러닝 라이브러리 파이토치(PyTorch) by 페이스북

## 1-2 코랩과 주피터 노트북

### 구글 코랩

: 웹 브라우저에서 무료로 파이썬 프로그램을 테스트하고 저장할 수 있는 서비스, 클라우드 기반의 주피터 노트북 개발 환경

- 노트북: 코랩 파일
- 주피터: 대화식 프로그래밍 환경
- 텍스트 셀 <br>
    ![image](https://github.com/user-attachments/assets/0c188f69-347e-4752-8f5a-3299b776d359)
    ![imgae](https://github.com/user-attachments/assets/a5404de3-6ec3-4ac1-ba57-939aeb923c55)

<br>

## 1-3 마켓과 머신러닝

- 머신러닝은 분류 기준을 스스로 찾아냄

### k-최근접 이웃 알고리즘(K-NN)

```python
# 도미와 빙어 데이터 합치기
length = bream_length+smelt_length
weight = bream_weight+smelt_weight
```

```python
# 데이터를 2차원 리스트로 만들기
fish_data = [[l, w] for l, w in zip(length, weight)]

print(fish_data)
```
- **2차원 리스트** <br>
: 머신러닝 패키지인 사이킷런을 사용하려면 각 특성의 리스트를 세로 방향으로 늘어뜨린 2차원 리스트(리스트의 리스트)를 만들어야함

- **zip()** <br>
: 나열된 리스트에서 원소를 하나씩 꺼내줌

    ![image](https://github.com/user-attachments/assets/3568f8be-4c22-416b-8cf1-bfb0d01864d8)

<br>

```python
# 정답 데이터 만들기
fish_target = [1]*35 + [0]*14
print(fish_target)
```
- 보통 찾으려는 대상을 1로 놓음 (여기서는 도미가 1)

```python
# KNeighborsClassifier 클래스의 객체 만들기
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
```
- **KNeighborsClassifier()** <br>
: k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스
    - **n_neighbors** 매개변수: 이웃의 개수 지정, 기본값 5
    - **p** 매개변수: 거리 재는 방법 지정, 기본값 2 (1이면 맨해튼 거리, 2이면 유클리디안 거리)
    - **n_jobs** 매개변수: 사용할 CPU 코어 지정, 기본값 1 (-1이면 모든 CPU 코어 사용, 이웃 간의 거리 계산 속도를 높일 수 있지만 fit() 메서드에는 영향x)

```python
# kn 모델 훈련시키기
kn.fit(fish_data, fish_target)
```
- **.fit()** <br>
: kn 객체에 fish_data와 fish_target을 전달하여 도미를 찾기 위한 기준을 학습시킴 -> 훈련

```python
# kn 모델 성능 평가하기(정확도)
kn.score(fish_data, fish_target)
```
- **.score()** <br>
: 0과 1 사이의 값 반환, 1은 모든 데이터를 정확히 맞혔다는 것 의미
    - 정확도 = (정확히 맞친 개수)/(전체 데이터 개수)

```python
# 새로운 데이터
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.scatter(30, 600, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
- KNN은 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용
![image](https://github.com/user-attachments/assets/38ef0a42-2c7d-4492-a7af-1904dd806bde) <br>
    삼각형으로 표시된 새로운 데이터는 도미라고 판단 <br>
    <- 주변에 도미 데이터가 많기 때문에(가장 가까운 직선거리에 어떤 데이터가 있는지)

```python
# 새로운 데이터의 정답 예측하기
kn.predict([[30, 600]])
```
- **.predict()**
: 새로운 데이터의 정답 예측 <br>
    - fit 메서드처럼 리스트의 리스트를 전달해야함 <br>
    - 이 예시에서는 array([1])을 반환하므로 해당 데이터는 도미

```python
# kn 모델의 내부 속성 확인하기
print(kn._fit_X)
print(kn._y)
```
- **._fit_X** <br>
: KNN 모델이 학습한 입력 데이터(특징 행렬, feature matrix)

- **_y** <br>
: KNN 모델이 학습한 출력 데이터(레이블, target vector)

- KNN의 특징
    - KNN에서의 "훈련"은 단순히 학습 데이터를 저장하는 과정으로 학습 데이터 자체가 모델의 일부로 저장되지만 
    - 훈련이 되는 모델에서는 학습 데이터를 직접 저장하기보다는 학습된 매개변수나 모델 구조를 저장함

```python
# 참고 데이터를 49개로 한 kn49 모델
kn49 = KNeighborsClassifier(n_neighbors=49)
```
- `n_neighbors='49'` <br>
: 기본값은 5개인데 49개로 바꿀 경우, 도미가 35개로 다수를 차지하므로 어떤 데이터를 넣어도 무조건 도미로 예측하게 됨

```python
# kn49 모델 성능 평가하기 (정확도)
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)
```
> 0.7142857142857143 = (도미 개수/전체 개수)

# Ch.2 데이터 다루기

## 2-1 훈련 세트와 테스트 세트

### 머신러닝 알고리즘의 종류

- 지도 학습 알고리즘
    -입력과 타깃을 전달하여 모델을 훈련한 다음, 새로운 데이터를 예측하는 데 활용
    - 훈련 데이터: 입력 + 타깃 <br>
        - ex) 입력: 생선의 길이와 무게, 타깃: 도미 여부
        - 특성: 길이와 무게
        - 훈련 세트와 테스트 세트로 분류

- 비지도 학습 알고리즘
    - 타깃이 없으며, 무엇을 예측하는 것이 아니라 입력에서 어떤 특징을 찾는 데 활용

### 훈련 세트와 테스트 세트

- **훈련 세트**
: 훈련에 사용되는 데이터

- **테스트 세트**
: 평가에 사용하는 데이터
    - 전체 데이터에서 20-30%
    - 데이터가 크다면 1%도 충분

- **샘플**
: 하나의 생선 데이터

    ![image](https://github.com/user-attachments/assets/06f1c61f-e666-4d63-890b-550473ef5df5) <br>

- **슬라이싱**
```python
# 훈련 세트로 입력값 중 0-34번째 인덱스까지 사용
train_input = fish_data[:35]
# 훈련 세트로 타깃값 중 0-34번째 인덱스까지 사용
train_target = fish_target[:35]
# 테스트 세트로 입력값 중 35번째부터 마지막 인덱스까지 사용
test_input = fish_data[35:]
# 테스트 세트로 타깃값 중 35번째부터 마지막 인덱스까지 사용
test_target = fish_target[35:]
```

### 샘플링 편향
: 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않음

- 위의 코드처럼 훈련 세트와 테스트 세트를 나누면 정확도가 0.0이 나옴
- 훈련 세트에 빙어가 하나도 없이 훈련하면 빙어를 올바르게 분류할 수 없기 때문
    - 훈련 세트에 도미만 있기 떄문에 테스트 세트가 무엇이든 무조건 도미로 분류
    - 테스트 세트에 빙어만 있기 때문에 정확도 0.0
- 도미와 빙어가 골고루 섞여야 함
![image](https://github.com/user-attachments/assets/d34877f0-aed4-4354-a696-ee51a8744749)
![image](https://github.com/user-attachments/assets/48a112eb-d5ae-428f-8850-1b96645274a6)

### 넘파이

: 파이썬의 대표적인 배열 라이브러리로 고차원 배열을 쉽게 만들 수 있음
![image](https://github.com/user-attachments/assets/ab3afebe-a339-4933-ad4d-a51757c08301) <br>

```python
# 파이썬 리스트를 넘파이 배열로 바꾸기
input_arr = np.array(fish_data)
target_arr = np.array(fish_target)
```
- **.array()**
: 파이썬 리스트를 넘파이 배열로 바꾸기
```python
# (샘플 수, 특성 수) 출력하기
print(input_arr.shape)
```
> (49, 2)
- **.shape** <br>
:(샘플 수, 특성 수) 출력

    ![image](https://github.com/user-attachments/assets/2b87c264-4a1b-47fa-980f-0a69e38d9eca) <br>

```python
# 인덱스 생성 후 무작위로 샘플 고르기
np.random.seed(42) 
index = np.arange(49)
np.random.shuffle(index)
```
- **.seed()**
: 넘파이에서 난수를 생성하기 위한 정수 초깃값 지정
    - 초깃값이 같으면 동일한 난수를 뽑을 수 있음
- **.arange()**
: 일정한 간격의 정수 또는 실수 배열 생성
    - 기본 간격은 1
    - ex) `np.arange(1, 3, 0.2)` <br>
        > [1., 1.2, 1.4, 1.6, 1.8, 2., 2.2, 2.4, 2.6, 2.8]
- **.shuffle()**
: 주어진 배열을 랜덤하게 섞음
    - 다차원 배열일 경우 첫 번째 축에 대해서만 섞음

    ![image](https://github.com/user-attachments/assets/7dbc7969-3bac-4063-b94a-2bcb6db19206) <br>

```python
# 훈련 세트의 입력값과 타깃값 생성
# shuffle된 index 배열의 처음 35개를 input_arr과 target_arr에 전달
train_input = input_arr[index[:35]]
train_target = target_arr[index[:35]]
```
```python
# 테스트 세트의 입력값과 타깃값 생성
# shuffle된 index 배열의 나머지 14개를 input_arr과 target_arr에 전달
test_input = input_arr[index[35:]]
test_target = target_arr[index[35:]]
```
```python
# 훈련 세트와 테스트 세트에 도미와 빙어가 잘 섞여 있는지 산점도 그리기
import matplotlib.pyplot as plt
plt.scatter(train_input[:, 0], train_input[:, 1])
plt.scatter(test_input[:, 0], test_input[:, 1])
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
- **plt.scatter** <br>
: 산점도 그리기
    - `train_input[:, 0]`-> x축: train_input 배열의 모든 행에서 첫 번째 열의 데이터(생선의 길이)
    - `train_input[:, 1]`-> y축: train_input 배열의 모든 행에서 두 번째 열의 데이터(생선의 무게)
![image](https://github.com/user-attachments/assets/e4cc8fc9-398f-4041-a0f0-08daa0b1f633)
- 파란색: 훈련 세트
- 주황색: 테스트 세트

    -> 잘 섞여 있음

```python
# 훈련 세트로 모델 훈련시키기
kn.fit(train_input, train_target)
# 테스트 세트로 모델 성능 평가하기
kn.score(test_input, test_target)
```
> 정확도 1.0

```python
# 테스트 세트의 예측 결과 확인
kn.predict(test_input)
# 테스트 세트의 실제 타깃 확인
test_target
```
> 둘 다 array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0])로 일치

- **array()** <br>
:넘파이 배열을 의미함

## 2-2 데이터 전처리

### 넘파이로 데이터 준비하기

```python
# 입력 데이터 만들기
import numpy as np
fish_data = np.column_stack((fish_length, fish_weight))
```
- **.column_stack()** <br>
: 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결
    - 연결할 리스트는 파이썬 튜플로 전달 <br>
![image](https://github.com/user-attachments/assets/c481ca6d-befd-48db-b8f4-9f4c6f15b112) <br>

```python
# 타깃 데이터 만들기
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
```
- **.concatenate()** <br>
: 첫 번째 차원을 따라 배열을 연결
![image](https://github.com/user-attachments/assets/ac28bbb9-365c-4243-aa00-1bb0a9c478f8)

### 사이킷런으로 훈련 세트와 테스트 세트 나누기

```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)
```
- **train_test_split()**
: 훈련 데이터를 훈련 세트와 테스트 세트로 나누는 함수
    - 알아서 데이터를 섞어줌
    - `test_size` 매개변수: 테스트 세트로 나눌 비율 지정, 기본값은 0.25(25%)
    - `random_state` 매개변수: 랜덤 시드 지정
    - `stratify` 매개변수: 클래스 비율(도미:빙어)에 맞게 데이터를 나눔 -> 샘플링 편향 해결
![image](https://github.com/user-attachments/assets/220e938d-aeff-4ed1-aa36-f2255b5e65ad)

### 수상한 도미 한 마리

```python
# kn 모델 훈련시키고 성능 평가하기
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```
> 정확도 1.0

```python
# 도미 데이터 넣고 결과 확인
print(kn.predict([[25, 150]]))
```
> [0.] <br>
> 문제점: 도미 데이터를 넣었는데 빙어로 분류

```python
import matplotlib.pyplot as plt
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![image](https://github.com/user-attachments/assets/bcadf946-efba-4fe3-a8a1-201456f6fecf)

> 육안으로 확인했을 때 도미 데이터에 가까움

```python
# 주어진 샘플에서 가까운 이웃들까지의 거리와 이웃 샘플들의 인덱스 반환
distances, indexes = kn.kneighbors([[25, 150]])

print(distances, indexes)
```
> [[ 92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]] [[21 33 19 30  1]]

- **.kneighbors()** <br>
: 입력한 데이터에 가장 가까운 이웃을 찾아 거리와 이웃 샘플의 인덱스 반환
    - `n_neighbors` 매개변수: 이웃의 개수 지정, 기본값은 객체 생성 시 지정한 개수
    - `return_distance` 매개변수: False로 지정 시 이웃 샘플의 인덱스만 반환하고 거리는 반환x, 기본값은 True

```python
# 삼각형 샘플에 가장 가까운 5개 샘플 표시
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![image](https://github.com/user-attachments/assets/69d07e67-6de4-4bc4-a36e-0d0d106fccdb) <br>

```python
print(train_input[indexes])
```
![image](https://github.com/user-attachments/assets/10aec6db-f2ff-4a75-aeaf-811d651422d9) <br>

```python
print(print(train_target[indexes]))
```
![image](https://github.com/user-attachments/assets/61451340-8f18-4e48-afd0-09d146222394) <br>

> 도미 1개 빙어 4개

### 기준을 맞춰라

![image](https://github.com/user-attachments/assets/c673a154-4e1f-4b1e-b3dc-8a559216c4d8)

> 문제점: 스케일이 다름

```python
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlim((0, 1000))
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

- **.xlim()** <br>
: x축 범위 지정

![image](https://github.com/user-attachments/assets/0bcb762d-e379-4f41-89a8-ef3fcb779fed)

- **데이터 전처리** <br>
: k-최근접 이웃처럼 특히 알고리즘이 거리 기반일 경우 데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 x -> 특성값을 일정한 기준으로 맞춰 주어야 함

- **표준점수(z 점수)** <br>
: 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타냄으로써 실제 특성값의 크기와 상관없이 동일한 조건으로 비교 가능

```python

mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0) # axis=0 특성별로 평균과 표준 편차 계산

print(mean, std)
```
> [ 27.29722222 454.09722222] [  9.98244253 323.29893931]

```python
# 브로드캐스팅
train_scaled = (train_input - mean) / std
```
![image](https://github.com/user-attachments/assets/775415d1-0502-41be-bc18-ef009b2a4718)

<br>

### 전처리 데이터로 모델 훈련하기

```python
# 주어진 샘플도 표준점수로 변환
new = ([25, 150] - mean) / std
```

```python
# kn 모델 훈련 시 스케일이 변환된 훈련 세트 사용
kn.fit(train_scaled, train_target)
```

```python
# kn 모델 성능 평가 시 스케일이 변환된 테스트 세트 사용
test_scaled = (test_input - mean) / std
kn.score(test_scaled, test_target)
```
> 정확도 1.0

```python
# 스케일이 변환된 주어진 샘플로 모델의 예측 출력하기
print(kn.predict([new]))
```
> 1.0
> 도미로 정확하게 예측함

```python
# 산점도로 가까운 이웃 확인하기
distances, indexes = kn.kneighbors([new])

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/dcba7717-dd14-4c27-89a4-e512f2478939) <br>
























