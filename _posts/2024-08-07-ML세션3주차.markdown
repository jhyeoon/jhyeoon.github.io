---
layout: post
title:  "ML세션 3주차"
date:   2024-08-07 19:37:24 +0800
categories: jekyll update
published: true
---

**# 혼자 공부하는 머신러닝+딥러닝 Ch. 3**

# Ch.3 회귀 알고리즘과 모델 규제

## 3-1 k-최근접 이웃 회귀

### **회귀**

- 지도 학습 알고리즘
    - 분류 (2장) 
    - **회귀 (3장)** 

- k-최근접 이웃 알고리즘
    - k-최근접 이웃 분류 알고리즘
        - 샘플의 타깃이 클래스
        - 가장 가까운 샘플 k개를 선택하여 클래스 확인 후, 다수의 클래스를 새로운 샘플의 클래스로 예측

    - **k-최근접 이웃 회귀 알고리즘**
        - 샘플의 타깃이 수치
        - 가장 가까운 샘플 k개를 선택하여 수치 확인 후, 수치들의 평균을 새로운 샘플의 수치로 예측

### **k-최근접 이웃 회귀**

**1. perch_length와 perch_weight 불러오기**
```python
import numpy as np

perch_length = np.array(
    [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0,
     21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5,
     22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5,
     27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0,
     36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0,
     40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
     )
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0,
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0,
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0,
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0,
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0,
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0,
     1000.0, 1000.0]
     )
```
<br>

**2. perch 데이터를 train set와 test set로 나누기**
```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42)
```

<br>

**3. train_input과 test_input 1차원 배열에서 열이 1개인 2차원 배열로 바꾸기**
```python
train_input = train_input.reshape(-1, 1)
test_input = test_input.reshape(-1, 1)
```
![image](https://github.com/user-attachments/assets/9ce502a0-e1b7-4987-9412-bbf6c558364a)

- **.reshape()**
: 배열의 크기 변형해주는 메서드
    - array.reshape( , )
    - reshape에서 -1
        - ex) 배열 x의 크기가 12일 때, x.reshape(-1, 3)을 하면 변경할 배열의 크기는 자동으로 (12/3 = 4, 3)
        - ex) 배열 x의 크기가 12일 때, x.reshape(2, -1)을 하면 차원의 크기는 자동으로 (2, 12/2 = 6)

<br>

**4. k-최근접 이웃 회귀 모델 훈련하기**
```python
from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)
```

<br>

**5. 모델 성능 평가하기**
```python
knr.score(test_input, test_target)
```
> 0.992809406101064

- **결정계수**(coefficient of determination)
    - 예측이 타깃 평균에 가까워지면 &rarr; score=0
    - 예측이 타깃에 가까워지면 &rarr; score=1
    - 1에 가까울수록 좋음

    $$R^2 = 1 - \frac{(타깃-예측)^2}{(타깃-타깃 평균)^2}$$

<br>

**6. 타깃과 예측한 값 사이의 차이 구하기**
```python
from sklearn.metrics import mean_absolute_error

# 테스트 세트에 대한 예측을 만듭니다
test_prediction = knr.predict(test_input)
# 테스트 세트에 대한 평균 절댓값 오차를 계산합니다
mae = mean_absolute_error(test_target, test_prediction)
print(mae)
```
> 19.157142857142862

&rarr; 예측이 평균적으로 19g 정도 타깃값과 다름

<br>

**7. 과대적합인지 과소적합인지 확인하기**

**과대적합 vs 과소적합**
- 과대적합 <br>
: 평가 시 훈련 세트를 사용했을 때의 점수 >> 평가 시 테스트 세트를 사용했을 때의 점수 <br>
- 과소적합 <br>
: 평가 시 훈련 세트를 사용했을 때의 점수 < 평가 시 테스트 세트를 사용했을 때의 점수 or 두 점수 모두 낮은 경우 <br>
    &larr; 모델이 너무 단순해서 훈련 세트로 적절히 훈련되지 않아서 발생 or 훈련 세트와 테스트 세트의 크기가 매우 작은 경우 발생

```python
print(knr.score(train_input, train_target))
```
> 0.9698823289099254

&rarr; 이 모델에서는 과소적합이 발생 <br>
&rarr; k의 개수를 줄임으로써 모델을 더 복잡하게 만들어야 함 <br>
    - 이웃의 개수를 줄이면 국지적 패턴에 민감해짐 <br>
    - 이웃의 개수를 늘리면 일반적인 패턴을 따름

<br>

**8. 과소적합 해결하기**
```python
# 이웃의 갯수를 3으로 설정합니다
knr.n_neighbors = 3
# 모델을 다시 훈련합니다
knr.fit(train_input, train_target)
print(knr.score(train_input, train_target))
```
> 0.9804899950518966

```python
print(knr.score(test_input, test_target))
```
> 0.9746459963987609

&rarr; 평가 시 훈련 세트를 사용했을 때의 점수 > 평가 시 테스트 세트를 사용했을 때의 점수 <br>
&rarr; 과소적합 해결!

## **3-2 선형 회귀**

- k-means 이웃 회귀의 문제점 <br>
: 훈련 데이터의 범위를 크게 벗어난 샘플도 그것과 가장 가까운 이웃들의 평균을 사용하기 때문에 실제 타깃과 예측값이 차이가 나게 됨

    - print(knr.predict([[50]]))
        > 1033.33333333333

        <img width="431" alt="image" src="https://github.com/user-attachments/assets/9cd8de27-27ab-433a-bb95-1dbb6865bfab">

    - print(knr.predict([[100]]))
        > 1033.33333333333

        <img width="431" alt="image" src="https://github.com/user-attachments/assets/38ea6886-2fde-4296-a48f-2fc5cec36312">

- 해결방안 <br>
: "선형 회귀" 사용

### **선형 회귀**
: 특성이 하나인 경우 어떤 직선을 학습하는 알고리즘

<br>

**1. 선형 회귀 모델 훈련하기**

```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression()

lr.fit(train_input, train_target)
```

<br>

**2. 50cm 농어의 무게 예측하기**

```python
print(lr.predict([[50]]))
```
> [1241.83860323]

<br>

**3. lr 모델의 파라미터 출력하기**

```python
print(lr.coef_, lr.intercept_)
```
> [39.01714496] -709.0186449535477

- **coef_** (기울기)
    - coefficient(계수)
    - weight(가중치)

- **intercept_** (절편)

> **머신러닝에서 "훈련"의 의미** <br> <br>

    - 모델 기반 학습 (선형 회귀) <br>
: 훈련 과정이 최적의 모델 파라미터를 찾는 것 <br> <br>
    - 사례 기반 학습 (k-최근접 이웃) <br>
: 훈련 과정이 훈련 세트를 저장하는 것

<br>

**4. 산점도와 회귀선 그리기**

```python
# 훈련 세트의 산점도를 그립니다
plt.scatter(train_input, train_target)
# 15에서 50까지 1차 방정식 그래프를 그립니다
plt.plot([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])
# 50cm 농어 데이터
plt.scatter(50, 1241.8, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="433" alt="image" src="https://github.com/user-attachments/assets/94ad1714-09c8-4de1-9ae2-7b6a7b3ec399">

<br>

**5. 훈련 세트와 테스트 세트의 $R^2$ 확인하기**

```python
print(lr.score(train_input, train_target))
print(lr.score(test_input, test_target))
```
> 0.939846333997604 <br>
> 0.8247503123313558

- 문제점 <br>
    -  훈련 세트의 점수 > 테스트 세트의 점수 <br>
        but 훈련 세트의 점수도 그닥 높지 않기 때문에 전체적으로 과소적합
    - 그래프의 왼쪽 아래 부분을 보면 농어의 무게가 0g 이하 <br>
        but 현실에서는 불가능
- 해결방안 <br>
    - 다항 회귀

### **다항 회귀**

$다항 회귀 \in 선형 회귀$ <br>
$\because$ '$길이^2=왕길이$'로 바꾸면 선형 관계로 표현 가능

<br>

**1. 이차방정식 만들기 위해 $(농어의 길이)^2$를 열에 추가**

```python
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))
```
```python
print(train_poly.shape, test_poly.shape)
```
> (42, 2) (14, 2)

<br>

**2. 다항 회귀 모델 훈련시키기**

```python
lr = LinearRegression()
lr.fit(train_poly, train_target)
```

<br>

**3. lr 모델의 파라미터 출력하기**

```python
print(lr.coef_, lr.intercept_)
```
> [  1.01433211 -21.55792498] 116.0502107827827

<br>

**4. 산점도와 회귀선 그리기**

```python
# 구간별 직선을 그리기 위해 15에서 49까지 정수 배열을 만듭니다
point = np.arange(15, 50)
# 훈련 세트의 산점도를 그립니다
plt.scatter(train_input, train_target)
# 15에서 49까지 2차 방정식 그래프를 그립니다
plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)
# 50cm 농어 데이터
plt.scatter([50], [1574], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="431" alt="image" src="https://github.com/user-attachments/assets/af1a42bd-6f7b-4bea-87a0-f18cd13a73c7">

<br>

**5. lr 모델 성능 평가하기**

```python
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```
> 0.9706807451768623 <br>
> 0.9775935108325122

&rarr; 훈련 세트와 테스트 세트에 대한 점수가 크게 높아짐 <br>
but 훈련 세트의 점수 < 테스트 세트의 점수 즉, 과소적합 <br>
&rarr; 더 복잡한 모델을 만들어 과소적합 해결해야 함

## **3-3 특성 공학과 규제**

- 회귀
    - 선형 회귀 (3-2)
    : 하나의 특성 사용 (길이)
    - **다중 회귀 (3-3)**
    : 여러 개의 특성 사용 (길이, 높이, 두께)

        <img width="386" alt="image" src="https://github.com/user-attachments/assets/b250f900-08e2-4020-a56d-49b7f3399ff8">

- 특성 공학 <br>
: 기존의 특성을 사용하여 새로운 특성을 뽑아내는 작업 <br>
ex) '농어 길이 X 농어 높이'

<br>

**1. 판다스로 농어 데이터 df에 저장하고, 넘파이 배열로 바꾸기**

```python
# 농어 입력 데이터 준비하기
import pandas as pd
df = pd.read_csv('https://bit.ly/perch_csv_data')
perch_full = df.to_numpy()
print(perch_full)

# 농어 타깃 데이터 준비하기
import numpy as np
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0,
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0,
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0,
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0,
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0,
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0,
     1000.0, 1000.0]
     )
```
- **판다스**
: 데이터 분석 라이브러리
    - 데이터프레임
    : 판다스의 핵심 데이터 구조
        - 넘파이 배열과 비슷하지만 더 많은 기능 제공

- **.read_csv**
: 판다스에서 csv읽기

- **.to_numpy**
: 넘파이 배열로 바꾸기

<br>

**2. perch_full과 perch_weight를 훈련 세트와 테스트 세트로 나누기**

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)
```

<br>

**3. 훈련 세트와 테스트 세트의 새로운 특성 만들기**

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(include_bias=False)

poly.fit(train_input)

train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
```
- **PolynomialFeatures()**
: 사이킷런의 변환기 클래스
    - LinearRegression()은 사이킷런의 추정기 클래스
    - `include_bias=False`: 특성에 추가된 절편 항 무시(사이킷런은 자동으로 해주지만 일단 이렇게 함)
- **.fit()**
: 새롭게 만들 특성 조합 찾음
- **.transform()**
: 실제로 데이터 변환

<br>

**4. 특성 확인하기**
```python
print(train_poly.shape)
poly.get_feature_names_out()
```
> (42, 9) <br>
> array(['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2','x2^2'], dtype=object)

- **.get_feature_names_out()**
: 각 특성들이 어떤 입력의 조합으로 만들어졌는지 알려줌

<br>

**5. 다중 회귀 모델 훈련하기**

```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train_poly, train_target)
```

<br>

**6. lr 모델 성능 평가하기**

```python
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```

> 0.9903183436982125 &rarr; 훈련 세트의 점수 &uarr; <br>
> 0.9714559911594111 &rarr; 과소적합 해결

<br>

**7. 5제곱까지 특성 만들기**

```python
poly = PolynomialFeatures(degree=5, include_bias=False)

poly.fit(train_input)

train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
```

<br>

**8. lr5 모델 훈련하기**

```python
lr5.fit(train_poly, train_target)
```

<br>

**9. lr5 모델 성능 평가하기**

```python
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```

> 0.9999999999996433 <br>
> -144.40579436844948 &rarr; 과대적합 발생

### 규제
: 과대적합을 훼방하는 것
- 선형 회귀 모델의 경우, 특성에 곱해지는 계수(기울기)를 작게 만드는 것
- 선형 회귀 모델에 규제를 추가한 모델 (규제하는 방법에 따라)
    - 릿지: 계수를 제곱한 값을 기준으로 규제 적용
        - 일반적으로 릿지 선호
    - 라쏘: 계수의 절댓값을 기준으로 규제 적용
        - 계수의 크기를 0으로 만들 수 있음

<img width="392" alt="image" src="https://github.com/user-attachments/assets/60b8e9f6-3ccd-4fe8-8c78-5254a93c0758">

**10. 스케일이 맞춰진 5제곱까지의 특성 만들기**

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)

train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```
- **StandardScaler()**
: 각 특성의 평균을 0으로, 표준 편차를 1로 조정하여 정규화
    - 앞에서 했던 것처럼 직접 평균과 표준편차를 구해 특성을 표준점수로 바꿀 필요 없음

### **릿지 회귀**

**11-1. 릿지 모델로 과대적합 해결하기**

```python
from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(train_scaled, train_target)

print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```
> 0.9896101671037343 <br>
> 0.9790693977615387 <br>
&rarr; 과대적합 해결! 

<br>

**11-2. 릿지 모델의 적합한 alpha 구하기**

- **alpha**
: 모델 객체를 만들 때 alpha 매개변수로 규제의 강도 조절해주는 매개변수
    - alpha값 찾는 방법 &rarr; alpha값에 대한 $R^2$값의 그래프 그려보기
    - 하이퍼파라미터
    : 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터

```python
import matplotlib.pyplot as plt

train_score = []
test_score = []

alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    # 릿지 모델을 만듭니다
    ridge = Ridge(alpha=alpha)
    # 릿지 모델을 훈련합니다
    ridge.fit(train_scaled, train_target)
    # 훈련 점수와 테스트 점수를 저장합니다
    train_score.append(ridge.score(train_scaled, train_target))
    test_score.append(ridge.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```
<img width="436" alt="image" src="https://github.com/user-attachments/assets/7d878c6a-a1b4-4771-97cd-17fe5640c202">

&rarr; alpha=0.1

<br>

**11-3. 찾은 alpha값 적용한 릿지 모델 성능 평가하기**

```python
ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)

print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```
&rarr; 모델 훈련 great!

### **라쏘 회귀**

**12-1. 라쏘 모델로 과대적합 해결하기**

```python
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```
> 0.989789897208096 <br>
> 0.9800593698421883

<br>

**12-2. 라쏘 모델의 적합한 alpha 구하기**

```python
train_score = []
test_score = []

alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    # 라쏘 모델을 만듭니다
    lasso = Lasso(alpha=alpha, max_iter=10000)
    # 라쏘 모델을 훈련합니다
    lasso.fit(train_scaled, train_target)
    # 훈련 점수와 테스트 점수를 저장합니다
    train_score.append(lasso.score(train_scaled, train_target))
    test_score.append(lasso.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```

<img width="431" alt="image" src="https://github.com/user-attachments/assets/f5cdd45b-cd8e-4490-8d47-d1c14ec5f459">

&rarr; alpha=10

<br>

**12-3. 찾은 alpha값 적용한 라쏘 모델 성능 평가하기**

```python
lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)

print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```

> 0.9888067471131867
> 0.9824470598706695

&rarr; 모델 훈련 great!

**12-4. 라쏘 모델에서 계수가 0인 개수 출력하기**

```python
print(np.sum(lasso.coef_ == 0))
```
> 40

&rarr; 계수 중 0이 40개 <br>
&rarr; 55개의 특성을 모델에 주입했지만 이 라쏘 모델이 사용한 특성은 15개 <br>
&rarr; 유용한 특성이 15개




































